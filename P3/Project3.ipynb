{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Udacity Data Analyst Nanodegree\n",
    "# P3: Data Wrangling with MongoDB\n",
    "\n",
    "Author: Luiz Gerosa\n",
    "\n",
    "Date: Mar 7, 2017\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "As this [New York Times article](https://www.nytimes.com/2014/08/18/technology/for-big-data-scientists-hurdle-to-insights-is-janitor-work.html) points out, the less heralded part of doing data science is manually collecting and cleaning data so it can be easily explored and analyzed later. Or otherwise known as “data wrangling” or “data munging” in the data science community.\n",
    "\n",
    "Though not as glamorous as building cool machine learning models, data wrangling is a task that data scientists can spend up to 50-80% of their time doing according to many practicing data analyst and data scientists.\n",
    "\n",
    "The goal of this project is to choose any area of the world in https://www.openstreetmap.org and use data munging techniques, such as assessing the quality of the data for validity, accuracy, completeness, consistency and uniformity, to clean the OpenStreetMap data.\n",
    "\n",
    "The following steps were executed in the wrangling process:\n",
    "1. Audit the data and fix some inconsistencies found \n",
    "1. Reshape the data from the OSM file (XML format) into a JSON format.\n",
    "1. Import the new file into MongoDB\n",
    "1. Explore the data using MongoDB\n",
    "\n",
    "This report outlines the results of the wrangling process, including:\n",
    "* Problems encountered in the map\n",
    "* Overview of the Data\n",
    "* Other ideas about the dataset\n",
    "\n",
    "### Map Area\n",
    "\n",
    "The metropolitan area choosen for this project was Vila Velha, Brazil.\n",
    "\n",
    "* [OpenStreetMap link](https://www.openstreetmap.org/relation/1825815)\n",
    "* [OSM file](https://s3.amazonaws.com/mapzen.odes/ex_fFDuD5tRsSCfuQwG1w9MHwL27gfpB.osm.bz2)\n",
    "\n",
    "### Tools used\n",
    "\n",
    "* Python 3.6.0\n",
    "* MongoDB 3.4.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problems Encountered\n",
    "The first step of the wrangling process is to audit the data to try to find and fix some inconsistencies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Street Types\n",
    "\n",
    "The code below go through the dataset to find over-abbreviated street types, e.g. \"Rod.\" rather than \"Rodovia\", or typos, e.g. \"Praca\" rather than \"Praça\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'set'>,\n",
      "            {'BAIRRO': {'BAIRRO PALMEIRAS'},\n",
      "             'BR': {'BR - 262', 'BR 262'},\n",
      "             'BR-101': {'BR-101'},\n",
      "             'BR-262': {'BR-262'},\n",
      "             'ESTRADA': {'ESTRADA 101 APOS POSTO Nossa Senhora do APARECIDA',\n",
      "                         'ESTRADA CAMBOAPINA',\n",
      "                         'ESTRADA PARA O ACQUAMANIA',\n",
      "                         'ESTRADA PARA PALMEIRAS'},\n",
      "             'FAZENDA': {'FAZENDA SAO DOMINGOS'},\n",
      "             'Graciano': {'Graciano Neves'},\n",
      "             'Mamoeiro': {'Mamoeiro'},\n",
      "             'Praca': {'Praca Henrique Meyerfreund'},\n",
      "             'RODOVIA': {'RODOVIA BR 262'},\n",
      "             'Rod': {'Rod BR-101 - Contorno'},\n",
      "             'Rod.': {'Rod. Es-080 - Governador José Sette'},\n",
      "             'Servidão': {'Servidão Antônio Luiz Curvacho 1',\n",
      "                          'Servidão Antônio Oliveira',\n",
      "                          'Servidão Cantídio Moreira',\n",
      "                          'Servidão Vitória Sant´Ana Ribeiro'},\n",
      "             'São': {'São José dos Calçados'}})\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.cElementTree as ET\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import pprint\n",
    "\n",
    "OSM_FILE = \"dataset/vila_velha.osm\"\n",
    "street_type_re = re.compile(r'^\\S+', re.IGNORECASE)\n",
    "\n",
    "expected = [\"Aeroporto\",\"Alameda\",\"Área\",\"Avenida\",\"Beco\",\"Campo\",\"Chácara\",\"Colônia\",\"Condomínio\",\"Conjunto\",\"Distrito\",\"Escadaria\",\"Esplanada\",\"Estação\",\"Estrada\",\"Favela\",\"Fazenda\",\"Feira\",\"Jardim\",\"Ladeira\",\"Lago\",\"Lagoa\",\"Largo\",\"Loteamento\",\"Morro\",\"Núcleo\",\"Parque\",\"Passarela\",\"Pátio\",\"Praça\",\"Quadra\",\"Rampa\",\"Recanto\",\"Residencial\",\"Rodovia\",\"Rua\",\"Setor\",\"Sítio\",\"Travessa\",\"Trecho\",\"Trevo\",\"Vale\",\"Vereda\",\"Via\",\"Viaduto\",\"Viela\",\"Vila\"]\n",
    "\n",
    "def audit_street_type(street_types, street_name):\n",
    "    m = street_type_re.search(street_name)\n",
    "    if m:\n",
    "        street_type = m.group()\n",
    "        if street_type not in expected:\n",
    "            street_types[street_type].add(street_name)\n",
    "\n",
    "\n",
    "def is_street_name(elem):\n",
    "    return (elem.attrib['k'] == \"addr:street\")\n",
    "\n",
    "def audit_street_types(osmfile):\n",
    "    street_types = defaultdict(set)\n",
    "    for event, elem in ET.iterparse(osmfile, events=(\"start\",)):\n",
    "\n",
    "        if elem.tag == \"node\" or elem.tag == \"way\":\n",
    "            for tag in elem.iter(\"tag\"):\n",
    "                if is_street_name(tag):\n",
    "                    audit_street_type(street_types, tag.attrib['v'])\n",
    "    return street_types\n",
    "\n",
    "st_types = audit_street_types(OSM_FILE)\n",
    "\n",
    "pprint.pprint(st_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `update_street_type()` fixes street types using a mapping. The function will be called later during the reshaping phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rod. Es-080 - Governador José Sette => Rodovia Es-080 - Governador José Sette\n",
      "ESTRADA CAMBOAPINA => Estrada CAMBOAPINA\n",
      "ESTRADA PARA O ACQUAMANIA => Estrada PARA O ACQUAMANIA\n",
      "ESTRADA PARA PALMEIRAS => Estrada PARA PALMEIRAS\n",
      "ESTRADA 101 APOS POSTO Nossa Senhora do APARECIDA => Estrada 101 APOS POSTO Nossa Senhora do APARECIDA\n",
      "RODOVIA BR 262 => Rodovia BR 262\n",
      "FAZENDA SAO DOMINGOS => Fazenda SAO DOMINGOS\n",
      "Praca Henrique Meyerfreund => Praça Henrique Meyerfreund\n",
      "Rod BR-101 - Contorno => Rodovia BR-101 - Contorno\n"
     ]
    }
   ],
   "source": [
    "mapping = { \"Rod\": \"Rodovia\",\n",
    "            \"Rod.\": \"Rodovia\",\n",
    "            \"RODOVIA\": \"Rodovia\",\n",
    "            \"Praca\": \"Praça\",\n",
    "            \"ESTRADA\": \"Estrada\",\n",
    "            \"FAZENDA\": \"Fazenda\"}\n",
    "\n",
    "def update_street_type(name):\n",
    "    m = street_type_re.search(name)\n",
    "    if m:\n",
    "        t = m.group()\n",
    "        if t in mapping:\n",
    "            return street_type_re.sub(mapping[t], name) \n",
    "    return name\n",
    "\n",
    "# validate if the function is working correctly\n",
    "for st_type, ways in st_types.items():\n",
    "    for name in ways:\n",
    "        fixed = update_street_type(name)\n",
    "        if name != fixed:\n",
    "            print(\"{} => {}\".format(name, fixed))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Zip Codes\n",
    "Brazilian zip code is called CEP and uses the format 00000-000. Also, acording to [Correios](http://www.correios.com.br/) web site, the range of valid zip codes for Vila Velha is between 29100-001 to 29129-999. The code below go through the dataset to find invalid zip codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zip codes with invalid format:\n",
      "{'29.053-245',\n",
      " '29.100-200',\n",
      " '29010340',\n",
      " '29010906',\n",
      " '29017950',\n",
      " '29041265',\n",
      " '29047850',\n",
      " '29050705',\n",
      " '29100000',\n",
      " '29102195',\n",
      " '29102240',\n",
      " '3149-7327'}\n",
      "\n",
      "Number of zip codes out of range: 258\n"
     ]
    }
   ],
   "source": [
    "zipcode_re = re.compile(r'^\\d{5}\\-\\d{3}$')\n",
    "digits_re = re.compile(r'\\d+')\n",
    "\n",
    "def extract_digits(str):\n",
    "    digits = digits_re.findall(str)\n",
    "    s = \"\"\n",
    "    for d in digits:\n",
    "        s += d\n",
    "    \n",
    "    return int(s)\n",
    "\n",
    "def is_zipcode(elem):\n",
    "    return (elem.attrib['k'] == \"addr:postcode\")\n",
    "\n",
    "def audit_zipcode(invalid_zipcodes, zipcode):\n",
    "    m = zipcode_re.search(zipcode)\n",
    "    if m is None:\n",
    "        invalid_zipcodes.add(zipcode)     \n",
    "\n",
    "def audit_zipcode_range(zipcodes_out_of_range, zipcode):\n",
    "    digits = extract_digits(zipcode)\n",
    "    if digits < 29101001 or digits > 29129999:\n",
    "        zipcodes_out_of_range.add(zipcode)\n",
    "\n",
    "def audit_zipcodes(osmfile):\n",
    "    invalid_zipcodes = set()\n",
    "    zipcodes_out_of_range = set()\n",
    "    for event, elem in ET.iterparse(osmfile, events=(\"start\",)):\n",
    "        if elem.tag == \"node\" or elem.tag == \"way\":\n",
    "            for tag in elem.iter(\"tag\"):\n",
    "                if is_zipcode(tag):\n",
    "                    audit_zipcode(invalid_zipcodes, tag.attrib['v'])\n",
    "                    audit_zipcode_range(zipcodes_out_of_range, tag.attrib['v'])\n",
    "    \n",
    "    return invalid_zipcodes, zipcodes_out_of_range\n",
    "\n",
    "invalid_zipcodes, zipcodes_out_of_range = audit_zipcodes(OSM_FILE)\n",
    "\n",
    "print(\"Zip codes with invalid format:\")\n",
    "pprint.pprint(invalid_zipcodes)\n",
    "print(\"\\nNumber of zip codes out of range: {}\".format(len(zipcodes_out_of_range)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `update_zipcode` standardize the zip codes to the CEP format and check if the zip code is within the valid range. This function will be called later during the reshape phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29047850 => None\n",
      "29.053-245 => None\n",
      "29102195 => 29102-195\n",
      "3149-7327 => None\n",
      "29.100-200 => None\n",
      "29010340 => None\n",
      "29100000 => None\n",
      "29050705 => None\n",
      "29102240 => 29102-240\n",
      "29017950 => None\n",
      "29041265 => None\n",
      "29010906 => None\n"
     ]
    }
   ],
   "source": [
    "def update_zipcode(zipcode):\n",
    "    digits = extract_digits(zipcode)\n",
    "    if digits < 29101001 or digits > 29129999:\n",
    "        return None\n",
    "    else:\n",
    "        fst_part = int(digits / 1000)\n",
    "        snd_part = round((digits/1000 - fst_part) * 1000)\n",
    "        return \"{}-{}\".format(fst_part, snd_part)\n",
    "\n",
    "# validate if the function is working correctly\n",
    "for invalid in invalid_zipcodes:\n",
    "    print(\"{} => {}\".format(invalid, update_zipcode(invalid)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshape Data to JSON\n",
    "To be able to import the data into MongoDB, the data has to be reshaped from XML to JSON format. The following rules were applied:\n",
    "* process only 2 types of top level tags: \"node\" and \"way\"\n",
    "* all attributes of \"node\" and \"way\" should be turned into regular key/value pairs, except:\n",
    "    - attributes in the CREATED array should be added under a key \"created\"\n",
    "    - attributes for latitude and longitude should be added to a \"pos\" array,\n",
    "      for use in geospacial indexing. Make sure the values inside \"pos\" array are floats\n",
    "      and not strings. \n",
    "* if the second level tag \"k\" value contains problematic characters, it should be ignored\n",
    "* if the second level tag \"k\" value starts with \"addr:\", it should be added to a dictionary \"address\"\n",
    "* if the second level tag \"k\" value does not start with \"addr:\", but contains \":\", you can\n",
    "  process it in a way that you feel is best. For example, you might split it into a two-level\n",
    "  dictionary like with \"addr:\", or otherwise convert the \":\" to create a valid key.\n",
    "* if there is a second \":\" that separates the type/direction of a street,\n",
    "  the tag should be ignored, for example:\n",
    "\n",
    "```xml\n",
    "<tag k=\"addr:housenumber\" v=\"5158\"/>\n",
    "<tag k=\"addr:street\" v=\"North Lincoln Avenue\"/>\n",
    "<tag k=\"addr:street:name\" v=\"Lincoln\"/>\n",
    "<tag k=\"addr:street:prefix\" v=\"North\"/>\n",
    "<tag k=\"addr:street:type\" v=\"Avenue\"/>\n",
    "<tag k=\"amenity\" v=\"pharmacy\"/>\n",
    "```\n",
    "\n",
    "should be turned into:\n",
    "\n",
    "```json\n",
    "{...\n",
    "\"address\": {\n",
    "    \"housenumber\": 5158,\n",
    "    \"street\": \"North Lincoln Avenue\"\n",
    "}\n",
    "\"amenity\": \"pharmacy\",\n",
    "...\n",
    "}\n",
    "```\n",
    "\n",
    "- for \"way\" specifically:\n",
    "\n",
    "  <nd ref=\"305896090\"/>\n",
    "  <nd ref=\"1719825889\"/>\n",
    "\n",
    "should be turned into\n",
    "```json\n",
    "\"node_refs\": [\"305896090\", \"1719825889\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import xml.etree.cElementTree as ET\n",
    "import pprint\n",
    "import re\n",
    "import io\n",
    "import json\n",
    "\n",
    "lower = re.compile(r'^([a-z]|_)*$')\n",
    "lower_colon = re.compile(r'^([a-z]|_)*:([a-z]|_)*$')\n",
    "problemchars = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')\n",
    "\n",
    "CREATED = [ \"version\", \"changeset\", \"timestamp\", \"user\", \"uid\"]\n",
    "ADDRESS_TAGS = [\"housenumber\", \"postcode\", \"street\", \"city\", \"place\", \"state\"]\n",
    "\n",
    "def shape_element(element):\n",
    "    \n",
    "    if element.tag == \"node\" or element.tag == \"way\" :\n",
    "        node = {\"type\" : element.tag}\n",
    "\n",
    "        # process the attributes\n",
    "        for k, v in element.attrib.items():\n",
    "            \n",
    "            # created\n",
    "            if k in CREATED:\n",
    "                if \"created\" not in node:\n",
    "                    node[\"created\"] = {}\n",
    "                node[\"created\"][k] = v\n",
    "            \n",
    "            # others\n",
    "            elif k != \"lat\" and k != \"lon\":\n",
    "                node[k] = v\n",
    "\n",
    "        # special attribute location\n",
    "        if element.tag == \"node\":\n",
    "            node[\"pos\"] = [float(element.attrib[\"lat\"]), float(element.attrib[\"lon\"])]\n",
    "\n",
    "        # process the 2nd level tags\n",
    "        for tag in element.iter(\"tag\"):\n",
    "            \n",
    "            k = tag.attrib[\"k\"]\n",
    "            v = tag.attrib[\"v\"]\n",
    "\n",
    "            if problemchars.search(k):\n",
    "                continue\n",
    "\n",
    "            if is_zipcode(tag):\n",
    "                v = update_zipcode(v)\n",
    "            elif is_street_name(tag):\n",
    "                v = update_street_type(v)\n",
    "\n",
    "            # tags that contains \":\"\n",
    "            split = k.split(\":\")\n",
    "            if len(split) == 1:\n",
    "                node[k] = v\n",
    "\n",
    "            elif len(split) == 2:\n",
    "\n",
    "                outer = split[0]\n",
    "                inner = split[1]\n",
    "\n",
    "                # address\n",
    "                if outer == \"addr\":\n",
    "                    if inner not in ADDRESS_TAGS:\n",
    "                        continue\n",
    "                    \n",
    "                    outer = \"address\"\n",
    "                    \n",
    "                    if outer not in node:\n",
    "                        node[outer] = {}\n",
    "                        \n",
    "                    node[outer][inner] = v\n",
    "                else:\n",
    "                    node[outer + \"_\" + inner] = v                \n",
    "        \n",
    "        # process node references\n",
    "        for nd in element.iter(\"nd\"):\n",
    "            ref = nd.attrib[\"ref\"]\n",
    "            if \"node_refs\" not in node:\n",
    "                node[\"node_refs\"] = []\n",
    "            node[\"node_refs\"].append(ref)\n",
    "\n",
    "\n",
    "        return node\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def process_map(file_in, pretty = False):\n",
    "    # You do not need to change this file\n",
    "    file_out = \"{0}.json\".format(file_in)\n",
    "    with io.open(file_out, \"w\", encoding=\"utf-8\") as fo:\n",
    "        for _, element in ET.iterparse(file_in):\n",
    "            el = shape_element(element)\n",
    "            if el:\n",
    "                indent = None\n",
    "                if pretty:\n",
    "                    indent = 2\n",
    "\n",
    "                fo.write(json.dumps(el, indent=indent, ensure_ascii=False)+\"\\n\")\n",
    "\n",
    "process_map(OSM_FILE, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data into MongoDB\n",
    "[`mongoimport`](https://docs.mongodb.com/manual/reference/program/mongoimport/) was used to import the JSON file into a local MongoDB instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "\n",
    "db_name = 'openstreetmap'\n",
    "\n",
    "# Connect to MongoDB\n",
    "client = MongoClient('localhost:27017')\n",
    "db = client[db_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping collection: vv\n",
      "Executing: mongoimport -h 127.0.0.1:27017 --db openstreetmap --collection vv --file dataset/vila_velha.osm.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "# Build mongoimport command\n",
    "json_file = OSM_FILE + '.json'\n",
    "\n",
    "collection = \"vv\"\n",
    "\n",
    "mongoimport_cmd = 'mongoimport -h 127.0.0.1:27017 ' + \\\n",
    "                  '--db ' + db_name + \\\n",
    "                  ' --collection ' + collection + \\\n",
    "                  ' --file ' + json_file\n",
    "\n",
    "# Before importing, drop collection if it is already running \n",
    "if collection in db.collection_names():\n",
    "    print('Dropping collection: ' + collection)\n",
    "    db[collection].drop()\n",
    "    \n",
    "# Execute the command\n",
    "print('Executing: ' + mongoimport_cmd)\n",
    "subprocess.call(mongoimport_cmd.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_id': ObjectId('58bf496db2e6d113c403e038'),\n",
      " 'created': {'changeset': '6563535',\n",
      "             'timestamp': '2010-12-06T15:31:54Z',\n",
      "             'uid': '31385',\n",
      "             'user': 'Skippern',\n",
      "             'version': '3'},\n",
      " 'id': '97464860',\n",
      " 'pos': [-20.3366098, -40.323035],\n",
      " 'type': 'node'}\n"
     ]
    }
   ],
   "source": [
    "# Check if the data was imported correctly\n",
    "pprint.pprint((db.vv.find_one()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Overview\n",
    "This section contains basic statistics about the dataset and the MongoDB queries used to gather them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The OSM file is 102.93 MB\n",
      "The JSON file is 115.64 MB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(\"The OSM file is {:4.2f} MB\".format(os.path.getsize(OSM_FILE)/1.0e6)) # convert from bytes to megabytes\n",
    "print(\"The JSON file is {:4.2f} MB\".format(os.path.getsize(json_file)/1.0e6)) # convert from bytes to megabytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "514356"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.vv.find().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "451964"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.vv.find({\"type\":\"node\"}).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of ways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62387"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.vv.find({\"type\":\"way\"}).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of unique users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "163"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(db.vv.distinct(\"created.user\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 10 contributing user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'_id': 'trilhado', 'count': 251140},\n",
      " {'_id': 'LucFreitas', 'count': 130318},\n",
      " {'_id': 'Skippern', 'count': 92471},\n",
      " {'_id': 'BladeTC', 'count': 14454},\n",
      " {'_id': 'Thundercel', 'count': 12908},\n",
      " {'_id': 'Társis José', 'count': 2994},\n",
      " {'_id': 'isadorarf', 'count': 1230},\n",
      " {'_id': 'erickdeoliveiraleal', 'count': 1008},\n",
      " {'_id': 'abel801', 'count': 947},\n",
      " {'_id': 'wolvsky', 'count': 706}]\n"
     ]
    }
   ],
   "source": [
    "top = db.vv.aggregate([\n",
    "    {\"$group\" : {\"_id\" : \"$created.user\", \"count\" : {\"$sum\" : 1}}},\n",
    "    {\"$sort\" : {\"count\": -1}},\n",
    "    {\"$limit\" : 10}])\n",
    "\n",
    "pprint.pprint((list(top)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Ideas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List of amenities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'_id': 'school', 'count': 330},\n",
      " {'_id': 'place_of_worship', 'count': 243},\n",
      " {'_id': 'parking', 'count': 174},\n",
      " {'_id': 'kindergarten', 'count': 134},\n",
      " {'_id': 'restaurant', 'count': 110},\n",
      " {'_id': 'clinic', 'count': 95},\n",
      " {'_id': 'fuel', 'count': 90},\n",
      " {'_id': 'bank', 'count': 78},\n",
      " {'_id': 'fast_food', 'count': 77},\n",
      " {'_id': 'pharmacy', 'count': 61},\n",
      " {'_id': 'police', 'count': 41},\n",
      " {'_id': 'hospital', 'count': 40},\n",
      " {'_id': 'community_centre', 'count': 28},\n",
      " {'_id': 'social_facility', 'count': 26},\n",
      " {'_id': 'public_building', 'count': 24},\n",
      " {'_id': 'university', 'count': 21},\n",
      " {'_id': 'doctors', 'count': 20},\n",
      " {'_id': 'prison', 'count': 19},\n",
      " {'_id': 'bench', 'count': 18},\n",
      " {'_id': 'college', 'count': 14},\n",
      " {'_id': 'bar', 'count': 14},\n",
      " {'_id': 'pub', 'count': 13},\n",
      " {'_id': 'bicycle_parking', 'count': 13},\n",
      " {'_id': 'toilets', 'count': 13},\n",
      " {'_id': 'car_wash', 'count': 12},\n",
      " {'_id': 'taxi', 'count': 12},\n",
      " {'_id': 'ice_cream', 'count': 12},\n",
      " {'_id': 'studio', 'count': 11},\n",
      " {'_id': 'cafe', 'count': 10},\n",
      " {'_id': 'veterinary', 'count': 10},\n",
      " {'_id': 'parking_entrance', 'count': 9},\n",
      " {'_id': 'theatre', 'count': 9},\n",
      " {'_id': 'bus_station', 'count': 9},\n",
      " {'_id': 'dentist', 'count': 9},\n",
      " {'_id': 'love_hotel', 'count': 9},\n",
      " {'_id': 'courthouse', 'count': 8},\n",
      " {'_id': 'shelter', 'count': 8},\n",
      " {'_id': 'post_office', 'count': 8},\n",
      " {'_id': 'atm', 'count': 7},\n",
      " {'_id': 'nightclub', 'count': 7},\n",
      " {'_id': 'townhall', 'count': 7},\n",
      " {'_id': 'telephone', 'count': 6},\n",
      " {'_id': 'marketplace', 'count': 6},\n",
      " {'_id': 'drinking_water', 'count': 6},\n",
      " {'_id': 'waste_basket', 'count': 4},\n",
      " {'_id': 'recycling', 'count': 4},\n",
      " {'_id': 'library', 'count': 4},\n",
      " {'_id': 'register_office', 'count': 3},\n",
      " {'_id': 'car_rental', 'count': 3},\n",
      " {'_id': 'fountain', 'count': 3},\n",
      " {'_id': 'fire_station', 'count': 2},\n",
      " {'_id': 'vending_machine', 'count': 2},\n",
      " {'_id': 'driving_school', 'count': 2},\n",
      " {'_id': 'grave_yard', 'count': 2},\n",
      " {'_id': 'nursing_home', 'count': 2},\n",
      " {'_id': 'arts_centre', 'count': 1},\n",
      " {'_id': 'embassy', 'count': 1},\n",
      " {'_id': 'boat_rental', 'count': 1},\n",
      " {'_id': 'convention_centre', 'count': 1},\n",
      " {'_id': 'ferry_terminal;bus_station', 'count': 1},\n",
      " {'_id': 'brothel', 'count': 1},\n",
      " {'_id': 'cinema', 'count': 1},\n",
      " {'_id': 'sw', 'count': 1},\n",
      " {'_id': 'conference_centre', 'count': 1},\n",
      " {'_id': 'social_centre', 'count': 1},\n",
      " {'_id': 'motorcycle_parking', 'count': 1}]\n"
     ]
    }
   ],
   "source": [
    "top = db.vv.aggregate([\n",
    "                   {\"$match\" : {\"amenity\" : {\"$exists\" : 1}}},\n",
    "                   {\"$group\" : {\"_id\" : \"$amenity\", \"count\" : {\"$sum\" : 1}}},\n",
    "                   {\"$sort\" : {\"count\" : -1}}])\n",
    "\n",
    "pprint.pprint((list(top)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 5 most popular cuisines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'_id': None, 'count': 46},\n",
      " {'_id': 'regional', 'count': 19},\n",
      " {'_id': 'pizza', 'count': 13},\n",
      " {'_id': 'steak_house', 'count': 7},\n",
      " {'_id': 'italian', 'count': 5}]\n"
     ]
    }
   ],
   "source": [
    "top = db.vv.aggregate([\n",
    "                   {\"$match\" : {\"amenity\":\"restaurant\"}},\n",
    "                   {\"$group\" : {\"_id\" : \"$cuisine\", \"count\" : {\"$sum\" : 1}}},\n",
    "                   {\"$sort\" : {\"count\" : -1}},\n",
    "                   {\"$limit\": 5}])\n",
    "pprint.pprint(list(top))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improving cuisine information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Restaurants without cuisine defined: 41.82%\n"
     ]
    }
   ],
   "source": [
    "cuisines = list(db.vv.aggregate([\n",
    "                   {\"$match\" : {\"amenity\":\"restaurant\"}},\n",
    "                   {\"$group\" : {\"_id\" : \"$cuisine\", \"count\" : {\"$sum\" : 1}}},\n",
    "                   {\"$sort\" : {\"count\" : -1}}]))\n",
    "\n",
    "total = sum([c[\"count\"] for c in cuisines])\n",
    "\n",
    "print(\"\\nRestaurants without cuisine defined: {:4.2f}%\".format(cuisines[0]['count'] / total * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be observed, more than 40% of the restaurants don't have the cuisine field defined. This is an issue if this dataset would be used, for instance, by a restaurant recommendation app.\n",
    "\n",
    "One way to improve the quality of the dataset is to try to infer the cuisine from the restaurant name. The query below lists the name of all restaurants without cuisine.\n",
    "\n",
    "For example, if the restaurant has \"Natural\" on its name, we could infer it's a vegetarian restaurant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Grelhados e Massas Restaurante',\n",
      " 'Saborear',\n",
      " 'Sabor Natural',\n",
      " 'Restaurante Porteira',\n",
      " 'Restaurante Moenda',\n",
      " 'Restaurante Sol & Mar',\n",
      " 'Restaurante Perfil',\n",
      " \"Restaurante Deboni's\",\n",
      " 'VitaNutri',\n",
      " 'Restaurante Bom Apetite',\n",
      " 'Lagoa Grande Restaurante',\n",
      " 'Restaurante Vista da Mata',\n",
      " 'Comida Gostosa Self Service',\n",
      " 'Restaurante Villa Augusta',\n",
      " 'Lanchonete e Restaurante Schwanz',\n",
      " 'Outback',\n",
      " 'Restaurante Vegetariano',\n",
      " 'Restaurante Paulista',\n",
      " 'Sabor Riviera Restaurante e Self-Service',\n",
      " 'Restaurante Cantinho do Sabor',\n",
      " 'Terra do Sabor Restaurante e Self-Service',\n",
      " 'Belas Ondas Restaurante',\n",
      " 'Galantina Self Service',\n",
      " 'Trindade',\n",
      " 'Fast Food Maná',\n",
      " 'Tangerina',\n",
      " 'Vila Mix Gourmet',\n",
      " 'Recreio Restaurante',\n",
      " 'Q.Luxo Self Service',\n",
      " 'Ilha do Caranguejo',\n",
      " 'La Rafa',\n",
      " 'Bifão Steakhouse',\n",
      " 'Spetacollo',\n",
      " 'Argento Parrilla',\n",
      " 'Brazillian Burger',\n",
      " 'Restaurante Mar e Terra',\n",
      " 'Restaurante Popular',\n",
      " 'Restaurante da Tânia',\n",
      " 'Restaurante Dona Bá',\n",
      " 'Espaço Sorriso',\n",
      " 'Timoneiro Restaurante',\n",
      " 'Simonatto']\n"
     ]
    }
   ],
   "source": [
    "query = {\"amenity\":\"restaurant\", \"cuisine\": {\"$exists\" : 0}, \"name\": {\"$exists\" : 1}}\n",
    "projection = {\"_id\" : 0, \"name\" : 1}\n",
    "restaurants_without_cuisine = [r[\"name\"] for r in db.vv.find(query, projection)]\n",
    "\n",
    "pprint.pprint(restaurants_without_cuisine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grelhados e Massas Restaurante => steak_house\n",
      "Sabor Natural => vegetarian\n",
      "Outback => steak_house\n",
      "Restaurante Vegetariano => vegetarian\n",
      "Belas Ondas Restaurante => seafood\n",
      "Ilha do Caranguejo => seafood\n",
      "Bifão Steakhouse => steak_house\n",
      "Brazillian Burger => burger\n"
     ]
    }
   ],
   "source": [
    "name_cuisine_mapping = {\n",
    "    \"Grelhados\" : \"steak_house\",\n",
    "    \"Natural\" : \"vegetarian\",\n",
    "    \"Outback\" : \"steak_house\",\n",
    "    \"Vegetariano\" : \"vegetarian\",\n",
    "    \"Caranguejo\" : \"seafood\",\n",
    "    \"Burger\" : \"burger\",\n",
    "    \"Steakhouse\" : \"steak_house\",\n",
    "    \"Ondas\" : \"seafood\"}\n",
    "\n",
    "for name in restaurants_without_cuisine:\n",
    "    for k, v in name_cuisine_mapping.items():\n",
    "        if k in name:\n",
    "            print(\"{} => {}\".format(name, v))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
