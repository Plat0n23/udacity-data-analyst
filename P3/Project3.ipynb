{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>jQuery(function() {if (jQuery(\"body.notebook_app\").length == 0) { jQuery(\".input_area\").toggle(); jQuery(\".prompt\").toggle();}});</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onclick=\"jQuery('.input_area').toggle(); jQuery('.prompt').toggle();\">Toggle code</button>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import IPython.core.display as di\n",
    "\n",
    "# This line will hide code by default when the notebook is exported as HTML\n",
    "di.display_html('<script>jQuery(function() {if (jQuery(\"body.notebook_app\").length == 0) { jQuery(\".input_area\").toggle(); jQuery(\".prompt\").toggle();}});</script>', raw=True)\n",
    "\n",
    "# This line will add a button to toggle visibility of code blocks, for use with the HTML export version\n",
    "di.display_html('''<button onclick=\"jQuery('.input_area').toggle(); jQuery('.prompt').toggle();\">Toggle code</button>''', raw=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Udacity Data Analyst Nanodegree\n",
    "# P3: Data Wrangling with MongoDB\n",
    "\n",
    "Author: Luiz Gerosa\n",
    "\n",
    "Date: Mar 7, 2017\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "As this [New York Times article](https://www.nytimes.com/2014/08/18/technology/for-big-data-scientists-hurdle-to-insights-is-janitor-work.html) points out, the less heralded part of doing data science is manually collecting and cleaning data so it can be easily explored and analyzed later. Or otherwise known as “data wrangling” or “data munging” in the data science community.\n",
    "\n",
    "Though not as glamorous as building cool machine learning models, data wrangling is a task that data scientists can spend up to 50-80% of their time doing according to many practicing data analyst and data scientists.\n",
    "\n",
    "The goal of this project is to choose any area of the world in https://www.openstreetmap.org and use data munging techniques, such as assessing the quality of the data for validity, accuracy, completeness, consistency and uniformity, to clean the OpenStreetMap data.\n",
    "\n",
    "The following steps were executed in the wrangling process:\n",
    "1. Audit the data and fix some inconsistencies found \n",
    "1. Reshape the data from the OSM file (XML format) into a JSON format.\n",
    "1. Import the new file into MongoDB\n",
    "1. Explore the data using MongoDB\n",
    "\n",
    "This report outlines the results of the wrangling process, including:\n",
    "* Problems encountered in the map\n",
    "* Overview of the Data\n",
    "* Other ideas about the dataset\n",
    "\n",
    "### Map Area\n",
    "\n",
    "The metropolitan area chosen for this project was Vila Velha, Brazil.\n",
    "\n",
    "* [OpenStreetMap link](https://www.openstreetmap.org/relation/1825815)\n",
    "* [OSM file](https://s3.amazonaws.com/mapzen.odes/ex_fFDuD5tRsSCfuQwG1w9MHwL27gfpB.osm.bz2)\n",
    "\n",
    "### Tools used\n",
    "\n",
    "* Python 3.6.0\n",
    "* MongoDB 3.4.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problems Encountered\n",
    "The first step of the wrangling process is to audit the data to try to find and fix some inconsistencies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Street Types\n",
    "\n",
    "The first issue found in the dataset was over-abbreviated street types, e.g. \"Rod.\" rather than \"Rodovia\", or typos, e.g. \"Praca\" rather than \"Praça\". The list below shows all occurences of streets that don't start with the expected types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'set'>,\n",
      "            {'BAIRRO': {'BAIRRO PALMEIRAS'},\n",
      "             'BR': {'BR 262', 'BR - 262'},\n",
      "             'BR-101': {'BR-101'},\n",
      "             'BR-262': {'BR-262'},\n",
      "             'ESTRADA': {'ESTRADA 101 APOS POSTO Nossa Senhora do APARECIDA',\n",
      "                         'ESTRADA CAMBOAPINA',\n",
      "                         'ESTRADA PARA O ACQUAMANIA',\n",
      "                         'ESTRADA PARA PALMEIRAS'},\n",
      "             'FAZENDA': {'FAZENDA SAO DOMINGOS'},\n",
      "             'Graciano': {'Graciano Neves'},\n",
      "             'Mamoeiro': {'Mamoeiro'},\n",
      "             'Praca': {'Praca Henrique Meyerfreund'},\n",
      "             'RODOVIA': {'RODOVIA BR 262'},\n",
      "             'Rod': {'Rod BR-101 - Contorno'},\n",
      "             'Rod.': {'Rod. Es-080 - Governador José Sette'},\n",
      "             'Servidão': {'Servidão Antônio Luiz Curvacho 1',\n",
      "                          'Servidão Antônio Oliveira',\n",
      "                          'Servidão Cantídio Moreira',\n",
      "                          'Servidão Vitória Sant´Ana Ribeiro'},\n",
      "             'São': {'São José dos Calçados'}})\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.cElementTree as ET\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import pprint\n",
    "\n",
    "OSM_FILE = \"dataset/vila_velha.osm\"\n",
    "street_type_re = re.compile(r'^\\S+', re.IGNORECASE)\n",
    "\n",
    "expected = [\"Aeroporto\",\"Alameda\",\"Área\",\"Avenida\",\"Beco\",\"Campo\",\"Chácara\",\"Colônia\",\"Condomínio\",\"Conjunto\",\"Distrito\",\"Escadaria\",\"Esplanada\",\"Estação\",\"Estrada\",\"Favela\",\"Fazenda\",\"Feira\",\"Jardim\",\"Ladeira\",\"Lago\",\"Lagoa\",\"Largo\",\"Loteamento\",\"Morro\",\"Núcleo\",\"Parque\",\"Passarela\",\"Pátio\",\"Praça\",\"Quadra\",\"Rampa\",\"Recanto\",\"Residencial\",\"Rodovia\",\"Rua\",\"Setor\",\"Sítio\",\"Travessa\",\"Trecho\",\"Trevo\",\"Vale\",\"Vereda\",\"Via\",\"Viaduto\",\"Viela\",\"Vila\"]\n",
    "\n",
    "def audit_street_type(street_types, street_name):\n",
    "    m = street_type_re.search(street_name)\n",
    "    if m:\n",
    "        street_type = m.group()\n",
    "        if street_type not in expected:\n",
    "            street_types[street_type].add(street_name)\n",
    "\n",
    "\n",
    "def is_street_name(elem):\n",
    "    return (elem.attrib['k'] == \"addr:street\")\n",
    "\n",
    "def audit_street_types(osmfile):\n",
    "    street_types = defaultdict(set)\n",
    "    for event, elem in ET.iterparse(osmfile, events=(\"start\",)):\n",
    "\n",
    "        if elem.tag == \"node\" or elem.tag == \"way\":\n",
    "            for tag in elem.iter(\"tag\"):\n",
    "                if is_street_name(tag):\n",
    "                    audit_street_type(street_types, tag.attrib['v'])\n",
    "    return street_types\n",
    "\n",
    "st_types = audit_street_types(OSM_FILE)\n",
    "\n",
    "pprint.pprint(st_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some street types were fixed using a dict mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mapping:\n",
      "\n",
      "{'ESTRADA': 'Estrada',\n",
      " 'FAZENDA': 'Fazenda',\n",
      " 'Praca': 'Praça',\n",
      " 'RODOVIA': 'Rodovia',\n",
      " 'Rod': 'Rodovia',\n",
      " 'Rod.': 'Rodovia'}\n",
      "\n",
      "Results after fixing the streets:\n",
      "\n",
      "Rod. Es-080 - Governador José Sette => Rodovia Es-080 - Governador José Sette\n",
      "ESTRADA 101 APOS POSTO Nossa Senhora do APARECIDA => Estrada 101 APOS POSTO Nossa Senhora do APARECIDA\n",
      "ESTRADA PARA PALMEIRAS => Estrada PARA PALMEIRAS\n",
      "ESTRADA PARA O ACQUAMANIA => Estrada PARA O ACQUAMANIA\n",
      "ESTRADA CAMBOAPINA => Estrada CAMBOAPINA\n",
      "RODOVIA BR 262 => Rodovia BR 262\n",
      "FAZENDA SAO DOMINGOS => Fazenda SAO DOMINGOS\n",
      "Praca Henrique Meyerfreund => Praça Henrique Meyerfreund\n",
      "Rod BR-101 - Contorno => Rodovia BR-101 - Contorno\n"
     ]
    }
   ],
   "source": [
    "mapping = { \"Rod\": \"Rodovia\",\n",
    "            \"Rod.\": \"Rodovia\",\n",
    "            \"RODOVIA\": \"Rodovia\",\n",
    "            \"Praca\": \"Praça\",\n",
    "            \"ESTRADA\": \"Estrada\",\n",
    "            \"FAZENDA\": \"Fazenda\"}\n",
    "\n",
    "print('\\nMapping:\\n')\n",
    "pprint.pprint(mapping)\n",
    "\n",
    "def update_street_type(name, mapping):\n",
    "    \"\"\"Fixes the street type (i.e the first word) of a street using a dict mapping. \n",
    "    For example, if name == \"Rod. BR-101\" and mapping == {\"Rod\" : \"Rodovia\"}, this function returns \"Rodovia BR-101\"\n",
    "    Args:\n",
    "        name: the street name.\n",
    "        mapping: the dict mapping used to fix the street types\n",
    "        \n",
    "    Returns:\n",
    "        the street name replacing the street type by its corrected version.\n",
    "    \"\"\"\n",
    "    m = street_type_re.search(name)\n",
    "    if m:\n",
    "        t = m.group()\n",
    "        if t in mapping:\n",
    "            return street_type_re.sub(mapping[t], name) \n",
    "    return name\n",
    "\n",
    "print('\\nResults after fixing the streets:\\n')\n",
    "# validate if the function is working correctly\n",
    "for st_type, ways in st_types.items():\n",
    "    for name in ways:\n",
    "        fixed = update_street_type(name, mapping)\n",
    "        if name != fixed:\n",
    "            print(\"{} => {}\".format(name, fixed))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Zip Codes\n",
    "Brazilian zip code is called CEP and uses the format 00000-000. Also, acording to [Correios](http://www.correios.com.br/) web site, the range of valid zip codes for Vila Velha is between 29100-001 to 29129-999."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zip codes with invalid format:\n",
      "{'29.053-245',\n",
      " '29.100-200',\n",
      " '29010340',\n",
      " '29010906',\n",
      " '29017950',\n",
      " '29041265',\n",
      " '29047850',\n",
      " '29050705',\n",
      " '29100000',\n",
      " '29102195',\n",
      " '29102240',\n",
      " '3149-7327'}\n",
      "\n",
      "Number of zip codes out of range: 258\n"
     ]
    }
   ],
   "source": [
    "zipcode_re = re.compile(r'^\\d{5}\\-\\d{3}$')\n",
    "digits_re = re.compile(r'\\d+')\n",
    "\n",
    "def extract_digits(str):\n",
    "    digits = digits_re.findall(str)\n",
    "    s = \"\"\n",
    "    for d in digits:\n",
    "        s += d\n",
    "    \n",
    "    return int(s)\n",
    "\n",
    "def is_zipcode(elem):\n",
    "    return (elem.attrib['k'] == \"addr:postcode\")\n",
    "\n",
    "def audit_zipcode(invalid_zipcodes, zipcode):\n",
    "    m = zipcode_re.search(zipcode)\n",
    "    if m is None:\n",
    "        invalid_zipcodes.add(zipcode)     \n",
    "\n",
    "def audit_zipcode_range(zipcodes_out_of_range, zipcode):\n",
    "    digits = extract_digits(zipcode)\n",
    "    if digits < 29101001 or digits > 29129999:\n",
    "        zipcodes_out_of_range.add(zipcode)\n",
    "\n",
    "def audit_zipcodes(osmfile):\n",
    "    invalid_zipcodes = set()\n",
    "    zipcodes_out_of_range = set()\n",
    "    for event, elem in ET.iterparse(osmfile, events=(\"start\",)):\n",
    "        if elem.tag == \"node\" or elem.tag == \"way\":\n",
    "            for tag in elem.iter(\"tag\"):\n",
    "                if is_zipcode(tag):\n",
    "                    audit_zipcode(invalid_zipcodes, tag.attrib['v'])\n",
    "                    audit_zipcode_range(zipcodes_out_of_range, tag.attrib['v'])\n",
    "    \n",
    "    return invalid_zipcodes, zipcodes_out_of_range\n",
    "\n",
    "invalid_zipcodes, zipcodes_out_of_range = audit_zipcodes(OSM_FILE)\n",
    "\n",
    "print(\"Zip codes with invalid format:\")\n",
    "pprint.pprint(invalid_zipcodes)\n",
    "print(\"\\nNumber of zip codes out of range: {}\".format(len(zipcodes_out_of_range)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The zip codes were standardized to the CEP format and checked if it's within the valid range. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29102195 => 29102-195\n",
      "3149-7327 => None\n",
      "29.053-245 => None\n",
      "29010340 => None\n",
      "29100000 => None\n",
      "29041265 => None\n",
      "29017950 => None\n",
      "29.100-200 => None\n",
      "29050705 => None\n",
      "29047850 => None\n",
      "29102240 => 29102-240\n",
      "29010906 => None\n"
     ]
    }
   ],
   "source": [
    "def update_zipcode(zipcode):\n",
    "    digits = extract_digits(zipcode)\n",
    "    if digits < 29101001 or digits > 29129999:\n",
    "        return None\n",
    "    else:\n",
    "        fst_part = int(digits / 1000)\n",
    "        snd_part = round((digits/1000 - fst_part) * 1000)\n",
    "        return \"{}-{}\".format(fst_part, snd_part)\n",
    "\n",
    "# validate if the function is working correctly\n",
    "for invalid in invalid_zipcodes:\n",
    "    print(\"{} => {}\".format(invalid, update_zipcode(invalid)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshape Data to JSON\n",
    "To be able to import the data into MongoDB, the data had to be reshaped from XML to JSON format. The following rules were applied:\n",
    "* process only 2 types of top level tags: \"node\" and \"way\"\n",
    "* all attributes of \"node\" and \"way\" should be turned into regular key/value pairs, except:\n",
    "    - attributes in the CREATED array should be added under a key \"created\"\n",
    "    - attributes for latitude and longitude should be added to a \"pos\" array,\n",
    "      for use in geospacial indexing. Make sure the values inside \"pos\" array are floats\n",
    "      and not strings. \n",
    "* if the second level tag \"k\" value contains problematic characters, it should be ignored\n",
    "* if the second level tag \"k\" value starts with \"addr:\", it should be added to a dictionary \"address\"\n",
    "* if the second level tag \"k\" value does not start with \"addr:\", but contains \":\", you can\n",
    "  process it in a way that you feel is best. For example, you might split it into a two-level\n",
    "  dictionary like with \"addr:\", or otherwise convert the \":\" to create a valid key.\n",
    "* if there is a second \":\" that separates the type/direction of a street,\n",
    "  the tag should be ignored, for example:\n",
    "\n",
    "```xml\n",
    "<tag k=\"addr:housenumber\" v=\"5158\"/>\n",
    "<tag k=\"addr:street\" v=\"North Lincoln Avenue\"/>\n",
    "<tag k=\"addr:street:name\" v=\"Lincoln\"/>\n",
    "<tag k=\"addr:street:prefix\" v=\"North\"/>\n",
    "<tag k=\"addr:street:type\" v=\"Avenue\"/>\n",
    "<tag k=\"amenity\" v=\"pharmacy\"/>\n",
    "```\n",
    "\n",
    "should be turned into:\n",
    "\n",
    "```json\n",
    "{...\n",
    "\"address\": {\n",
    "    \"housenumber\": 5158,\n",
    "    \"street\": \"North Lincoln Avenue\"\n",
    "}\n",
    "\"amenity\": \"pharmacy\",\n",
    "...\n",
    "}\n",
    "```\n",
    "\n",
    "- for \"way\" specifically:\n",
    "\n",
    "  <nd ref=\"305896090\"/>\n",
    "  <nd ref=\"1719825889\"/>\n",
    "\n",
    "should be turned into\n",
    "```json\n",
    "\"node_refs\": [\"305896090\", \"1719825889\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import xml.etree.cElementTree as ET\n",
    "import pprint\n",
    "import re\n",
    "import io\n",
    "import json\n",
    "\n",
    "lower = re.compile(r'^([a-z]|_)*$')\n",
    "lower_colon = re.compile(r'^([a-z]|_)*:([a-z]|_)*$')\n",
    "problemchars = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')\n",
    "\n",
    "CREATED = [ \"version\", \"changeset\", \"timestamp\", \"user\", \"uid\"]\n",
    "ADDRESS_TAGS = [\"housenumber\", \"postcode\", \"street\", \"city\", \"place\", \"state\"]\n",
    "\n",
    "def shape_element(element):\n",
    "    \n",
    "    if element.tag == \"node\" or element.tag == \"way\" :\n",
    "        node = {\"type\" : element.tag}\n",
    "\n",
    "        # process the attributes\n",
    "        for k, v in element.attrib.items():\n",
    "            \n",
    "            # created\n",
    "            if k in CREATED:\n",
    "                if \"created\" not in node:\n",
    "                    node[\"created\"] = {}\n",
    "                node[\"created\"][k] = v\n",
    "            \n",
    "            # others\n",
    "            elif k != \"lat\" and k != \"lon\":\n",
    "                node[k] = v\n",
    "\n",
    "        # special attribute location\n",
    "        if element.tag == \"node\":\n",
    "            node[\"pos\"] = [float(element.attrib[\"lat\"]), float(element.attrib[\"lon\"])]\n",
    "\n",
    "        # process the 2nd level tags\n",
    "        for tag in element.iter(\"tag\"):\n",
    "            \n",
    "            k = tag.attrib[\"k\"]\n",
    "            v = tag.attrib[\"v\"]\n",
    "\n",
    "            if problemchars.search(k):\n",
    "                continue\n",
    "\n",
    "            if is_zipcode(tag):\n",
    "                v = update_zipcode(v)\n",
    "            elif is_street_name(tag):\n",
    "                v = update_street_type(v, mapping)\n",
    "\n",
    "            # tags that contains \":\"\n",
    "            split = k.split(\":\")\n",
    "            if len(split) == 1:\n",
    "                node[k] = v\n",
    "\n",
    "            elif len(split) == 2:\n",
    "\n",
    "                outer = split[0]\n",
    "                inner = split[1]\n",
    "\n",
    "                # address\n",
    "                if outer == \"addr\":\n",
    "                    if inner not in ADDRESS_TAGS:\n",
    "                        continue\n",
    "                    \n",
    "                    outer = \"address\"\n",
    "                    \n",
    "                    if outer not in node:\n",
    "                        node[outer] = {}\n",
    "                        \n",
    "                    node[outer][inner] = v\n",
    "                else:\n",
    "                    node[outer + \"_\" + inner] = v                \n",
    "        \n",
    "        # process node references\n",
    "        for nd in element.iter(\"nd\"):\n",
    "            ref = nd.attrib[\"ref\"]\n",
    "            if \"node_refs\" not in node:\n",
    "                node[\"node_refs\"] = []\n",
    "            node[\"node_refs\"].append(ref)\n",
    "\n",
    "\n",
    "        return node\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def process_map(file_in, pretty = False):\n",
    "    # You do not need to change this file\n",
    "    file_out = \"{0}.json\".format(file_in)\n",
    "    with io.open(file_out, \"w\", encoding=\"utf-8\") as fo:\n",
    "        for _, element in ET.iterparse(file_in):\n",
    "            el = shape_element(element)\n",
    "            if el:\n",
    "                indent = None\n",
    "                if pretty:\n",
    "                    indent = 2\n",
    "\n",
    "                fo.write(json.dumps(el, indent=indent, ensure_ascii=False)+\"\\n\")\n",
    "\n",
    "process_map(OSM_FILE, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data into MongoDB\n",
    "[`mongoimport`](https://docs.mongodb.com/manual/reference/program/mongoimport/) was used to import the JSON file into a local MongoDB instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "\n",
    "db_name = 'openstreetmap'\n",
    "\n",
    "# Connect to MongoDB\n",
    "client = MongoClient('localhost:27017')\n",
    "db = client[db_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping collection: vv\n",
      "Executing: mongoimport -h 127.0.0.1:27017 --db openstreetmap --collection vv --file dataset/vila_velha.osm.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "# Build mongoimport command\n",
    "json_file = OSM_FILE + '.json'\n",
    "\n",
    "collection = \"vv\"\n",
    "\n",
    "mongoimport_cmd = 'mongoimport -h 127.0.0.1:27017 ' + \\\n",
    "                  '--db ' + db_name + \\\n",
    "                  ' --collection ' + collection + \\\n",
    "                  ' --file ' + json_file\n",
    "\n",
    "# Before importing, drop collection if it is already running \n",
    "if collection in db.collection_names():\n",
    "    print('Dropping collection: ' + collection)\n",
    "    db[collection].drop()\n",
    "    \n",
    "# Execute the command\n",
    "print('Executing: ' + mongoimport_cmd)\n",
    "subprocess.call(mongoimport_cmd.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Overview\n",
    "This section contains basic statistics about the dataset and the MongoDB queries used to gather them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The OSM file is 102.93 MB\n",
      "The JSON file is 115.64 MB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(\"The OSM file is {:4.2f} MB\".format(os.path.getsize(OSM_FILE)/1.0e6)) # convert from bytes to megabytes\n",
    "print(\"The JSON file is {:4.2f} MB\".format(os.path.getsize(json_file)/1.0e6)) # convert from bytes to megabytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "514356"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.vv.find().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "451964"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.vv.find({\"type\":\"node\"}).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of ways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62387"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.vv.find({\"type\":\"way\"}).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of unique users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "163"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(db.vv.distinct(\"created.user\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 10 contributing user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'_id': 'trilhado', 'count': 251140},\n",
      " {'_id': 'LucFreitas', 'count': 130318},\n",
      " {'_id': 'Skippern', 'count': 92471},\n",
      " {'_id': 'BladeTC', 'count': 14454},\n",
      " {'_id': 'Thundercel', 'count': 12908},\n",
      " {'_id': 'Társis José', 'count': 2994},\n",
      " {'_id': 'isadorarf', 'count': 1230},\n",
      " {'_id': 'erickdeoliveiraleal', 'count': 1008},\n",
      " {'_id': 'abel801', 'count': 947},\n",
      " {'_id': 'wolvsky', 'count': 706}]\n"
     ]
    }
   ],
   "source": [
    "top = db.vv.aggregate([\n",
    "    {\"$group\" : {\"_id\" : \"$created.user\", \"count\" : {\"$sum\" : 1}}},\n",
    "    {\"$sort\" : {\"count\": -1}},\n",
    "    {\"$limit\" : 10}])\n",
    "\n",
    "pprint.pprint((list(top)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Ideas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 5 most popular cuisines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'_id': None, 'count': 46},\n",
      " {'_id': 'regional', 'count': 19},\n",
      " {'_id': 'pizza', 'count': 13},\n",
      " {'_id': 'steak_house', 'count': 7},\n",
      " {'_id': 'italian', 'count': 5}]\n"
     ]
    }
   ],
   "source": [
    "top = db.vv.aggregate([\n",
    "                   {\"$match\" : {\"amenity\":\"restaurant\"}},\n",
    "                   {\"$group\" : {\"_id\" : \"$cuisine\", \"count\" : {\"$sum\" : 1}}},\n",
    "                   {\"$sort\" : {\"count\" : -1}},\n",
    "                   {\"$limit\": 5}])\n",
    "pprint.pprint(list(top))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improving cuisine information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Restaurants without cuisine defined: 41.82%\n"
     ]
    }
   ],
   "source": [
    "cuisines = list(db.vv.aggregate([\n",
    "                   {\"$match\" : {\"amenity\":\"restaurant\"}},\n",
    "                   {\"$group\" : {\"_id\" : \"$cuisine\", \"count\" : {\"$sum\" : 1}}},\n",
    "                   {\"$sort\" : {\"count\" : -1}}]))\n",
    "\n",
    "total = sum([c[\"count\"] for c in cuisines])\n",
    "\n",
    "print(\"\\nRestaurants without cuisine defined: {:4.2f}%\".format(cuisines[0]['count'] / total * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be observed, more than 40% of the restaurants don't have the cuisine field defined. This is an issue if this dataset would be used, for instance, by a restaurant recommendation app.\n",
    "\n",
    "One way to improve the quality of the dataset is to try to infer the cuisine from the restaurant name. For example, if the restaurant has \"Natural\" on its name, we could infer it's a vegetarian restaurant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping:\n",
      "\n",
      "{'Burger': 'burger',\n",
      " 'Caranguejo': 'seafood',\n",
      " 'Grelhados': 'steak_house',\n",
      " 'Mar': 'seafood',\n",
      " 'Natural': 'vegetarian',\n",
      " 'Ondas': 'seafood',\n",
      " 'Outback': 'steak_house',\n",
      " 'Steakhouse': 'steak_house',\n",
      " 'Terra': 'vegetarian',\n",
      " 'Vegetariano': 'vegetarian'}\n",
      "\n",
      "Cuisines inferred from the restaurant's name:\n",
      "\n",
      "Grelhados e Massas Restaurante => steak_house\n",
      "Sabor Natural => vegetarian\n",
      "Restaurante Sol & Mar => seafood\n",
      "Outback => steak_house\n",
      "Restaurante Vegetariano => vegetarian\n",
      "Terra do Sabor Restaurante e Self-Service => vegetarian\n",
      "Belas Ondas Restaurante => seafood\n",
      "Ilha do Caranguejo => seafood\n",
      "Bifão Steakhouse => steak_house\n",
      "Brazillian Burger => burger\n",
      "Restaurante Mar e Terra => seafood\n",
      "Restaurante Mar e Terra => vegetarian\n"
     ]
    }
   ],
   "source": [
    "query = {\"amenity\":\"restaurant\", \"cuisine\": {\"$exists\" : 0}, \"name\": {\"$exists\" : 1}}\n",
    "projection = {\"_id\" : 0, \"name\" : 1}\n",
    "restaurants_without_cuisine = [r[\"name\"] for r in db.vv.find(query, projection)]\n",
    "\n",
    "name_cuisine_mapping = {\n",
    "    \"Grelhados\" : \"steak_house\",\n",
    "    \"Natural\" : \"vegetarian\",\n",
    "    \"Outback\" : \"steak_house\",\n",
    "    \"Vegetariano\" : \"vegetarian\",\n",
    "    \"Caranguejo\" : \"seafood\",\n",
    "    \"Burger\" : \"burger\",\n",
    "    \"Steakhouse\" : \"steak_house\",\n",
    "    \"Ondas\" : \"seafood\",\n",
    "    \"Mar\" : \"seafood\",\n",
    "    \"Terra\": \"vegetarian\"}\n",
    "print('Mapping:\\n')\n",
    "pprint.pprint(name_cuisine_mapping)\n",
    "\n",
    "\n",
    "print('\\nCuisines inferred from the restaurant\\'s name:\\n')\n",
    "for name in restaurants_without_cuisine:\n",
    "    for k, v in name_cuisine_mapping.items():\n",
    "        if k in name:\n",
    "            print(\"{} => {}\".format(name, v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This approach has some issues that have to be addressed. \n",
    "\n",
    "For instance, \"Mar\" (sea in Portuguese) was mapped to \"seafood\" and \"Terra\" (soil in Portuguese) was mapped to \"vegetarian\". The restaurant \"Restaurante Mar e Terra\" has both words on its name and the classification could be wrong. In this case, we could define a prioritization in the mapping to classify to seafood rather than vegetarian, as vegetarian restaurants are not supposed to cook any kind of animal.\n",
    "\n",
    "Another problem could be the wrong classifications. For example, \"STK South Beach\" is a steak house in Miami that would be wrong classified as seafood because of the word \"Beach\". In this case we could use other sources like Google Maps or Facebook to enhance the dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
